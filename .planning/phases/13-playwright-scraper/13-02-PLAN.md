---
phase: 13-playwright-scraper
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - ___/accounts/scraper.py
autonomous: true

must_haves:
  truths:
    - "Scraper returns voting location for valid active cedula"
    - "Scraper returns cancelled status for deceased/cancelled cedula"
    - "Scraper returns not_found for invalid cedula"
    - "Scraper returns timeout status when page doesn't load in 90s"
    - "Scraper returns blocked status when F5 CSPM triggers"
    - "Scraper enforces minimum 5 seconds between requests"
  artifacts:
    - path: "___/accounts/scraper.py"
      provides: "Full scraping implementation with all response types"
      exports: ["RegistraduriaScraper", "scrape_cedula"]
      min_lines: 150
  key_links:
    - from: "___/accounts/scraper.py"
      to: "Registraduria page elements"
      via: "Locator selectors for form and results"
      pattern: "page\\.locator\\("
    - from: "___/accounts/scraper.py"
      to: "CedulaInfo.Status"
      via: "Status codes mapping"
      pattern: "status.*found|not_found|cancelled|blocked|timeout"
---

<objective>
Implement full scraping logic with all response types, error handling, and rate limiting.

Purpose: Complete the Registraduria census scraper that retrieves voting location data for any cedula.
Output: Fully functional scrape_cedula() that handles all scenarios (active, cancelled, not_found, timeout, blocked, error).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-playwright-scraper/13-CONTEXT.md
@.planning/phases/13-playwright-scraper/13-RESEARCH.md
@.planning/phases/13-playwright-scraper/13-01-SUMMARY.md

# Relevant source
@___/accounts/scraper.py (Plan 01 created base class)
@___/accounts/models.py (CedulaInfo.Status for reference)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement scrape_cedula with form submission and response parsing</name>
  <files>___/accounts/scraper.py</files>
  <action>
Update `scrape_cedula(self, cedula: str) -> dict` in RegistraduriaScraper to implement full scraping logic:

**1. Navigate to page:**
```python
page.goto(REGISTRADURIA_URL, timeout=30000, wait_until='domcontentloaded')
```

**2. Fill and submit form:**
- Locate cedula input field (inspect live page for exact selector)
- Fill with cedula value using `locator.fill(cedula)`
- Locate and click submit/Consultar button
- Common selectors to try: `input[type="text"]`, `input#cedula`, `button:has-text("Consultar")`

**3. Wait for result:**
```python
result_locator = page.locator('[selector for result area]')
result_locator.wait_for(state='visible', timeout=90000)
```

**4. Parse response - determine result type:**
Check page content to identify which response type:
- **Active cedula:** Look for "INFORMACION DEL LUGAR DE VOTACION" or voting location text
- **Cancelled cedula:** Look for "novedad", "cancelada", or similar indicators
- **Not found:** Look for error message about cedula not in census
- **Blocked:** Look for F5 challenge page or "access denied" patterns

**5. Extract data based on response type:**

For ACTIVE response, extract:
```python
{
    'status': 'found',
    'departamento': extract_field(page, '[selector]'),
    'municipio': extract_field(page, '[selector]'),
    'puesto': extract_field(page, '[selector]'),
    'direccion': extract_field(page, '[selector]'),
    'mesa': extract_field(page, '[selector]'),
}
```

For CANCELLED response:
```python
{
    'status': 'cancelled',
    'novedad': extract_field(page, '[selector]'),
    'resolucion': extract_field(page, '[selector]'),
    'fecha_novedad': extract_field(page, '[selector]'),
}
```

For NOT_FOUND:
```python
{'status': 'not_found'}
```

For BLOCKED (save raw HTML for debugging per CONTEXT.md):
```python
{'status': 'blocked', 'raw_html': page.content(), 'error': 'F5 CSPM challenge detected'}
```

**6. Add helper method `_extract_field(self, page, selector) -> str | None`:**
- Uses `page.locator(selector)`
- Returns stripped text_content or None if not found

**7. Handle exceptions:**
- Wrap main logic in try/except
- `PlaywrightTimeoutError` -> `{'status': 'timeout', 'error': str(e)}`
- Other exceptions -> `{'status': 'network_error', 'error': str(e)}`
- Parse failures -> `{'status': 'parse_error', 'raw_html': page.content(), 'error': str(e)}`

**8. Logging:**
- INFO level: `logger.info(f"Scrape {cedula}: {result['status']}")`
- DEBUG level: log extracted values on success
- ERROR level: log full error on failures

**Import needed:**
```python
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
```

**Selector discovery:**
The exact CSS selectors need to be determined by inspecting the live Registraduria page. Keep selectors as constants at module top for easy updates. If selectors can't be determined, use semantic selectors like `button:has-text("Consultar")` and add comments noting they may need adjustment.
  </action>
  <verify>
Run: `python manage.py shell -c "from accounts.scraper import RegistraduriaScraper; s = RegistraduriaScraper(); print(type(s.scrape_cedula('12345678')))"`
Should return: `<class 'dict'>`
The result dict should have 'status' key.
  </verify>
  <done>scrape_cedula() implements full scraping with all response type handling</done>
</task>

<task type="auto">
  <name>Task 2: Add rate limiting between requests</name>
  <files>___/accounts/scraper.py</files>
  <action>
Add rate limiting to enforce minimum 5 seconds between requests (per SCRP-07):

**1. Add class-level tracking:**
```python
_last_request_time: float = 0  # Class variable
```

**2. Add rate limiting method:**
```python
@classmethod
def _enforce_rate_limit(cls):
    """Ensure minimum 5 seconds between requests."""
    import time
    elapsed = time.time() - cls._last_request_time
    if elapsed < RATE_LIMIT_SECONDS:
        sleep_time = RATE_LIMIT_SECONDS - elapsed
        logger.debug(f"Rate limiting: sleeping {sleep_time:.1f}s")
        time.sleep(sleep_time)
    cls._last_request_time = time.time()
```

**3. Call at start of scrape_cedula:**
```python
def scrape_cedula(self, cedula: str) -> dict:
    self._enforce_rate_limit()
    # ... rest of scraping logic
```

**4. Import time module** at top of file.

This ensures that even if multiple tasks call scrape_cedula rapidly, each request waits at least 5 seconds from the previous one.
  </action>
  <verify>
Run: `python manage.py shell`
```python
from accounts.scraper import RegistraduriaScraper
import time
s = RegistraduriaScraper()
start = time.time()
s.scrape_cedula('12345678')
s.scrape_cedula('87654321')
elapsed = time.time() - start
print(f"Two calls took {elapsed:.1f}s (should be >= 5s)")
```
  </verify>
  <done>Rate limiting enforces minimum 5 seconds between scrape requests</done>
</task>

</tasks>

<verification>
1. scrape_cedula() returns dict with 'status' key
2. Response handling covers: found, not_found, cancelled, blocked, timeout, network_error, parse_error
3. Rate limiting enforces 5+ seconds between requests
4. Logging outputs at INFO level for status, DEBUG for details
5. raw_html captured only on error states (per CONTEXT.md)
6. No exceptions bubble up - all caught and converted to status dicts
</verification>

<success_criteria>
- scrape_cedula() handles all 7 response types from SCRP requirements
- Rate limiting enforces minimum 5 seconds between requests (SCRP-07)
- Errors are returned as status dicts, not raised as exceptions
- raw_html stored only on error for debugging
- Integration with django-q logger
- Ready for Phase 14 to wire into background tasks
</success_criteria>

<output>
After completion, create `.planning/phases/13-playwright-scraper/13-02-SUMMARY.md`
</output>
