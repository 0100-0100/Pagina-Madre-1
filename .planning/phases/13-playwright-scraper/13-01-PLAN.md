---
phase: 13-playwright-scraper
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - ___/accounts/scraper.py
autonomous: true

must_haves:
  truths:
    - "Playwright is installed and importable in Python"
    - "Chromium browser binary exists and launches"
    - "Browser singleton initializes lazily on first access"
    - "Browser context is created and cleaned up per scrape call"
  artifacts:
    - path: "requirements.txt"
      provides: "playwright dependency"
      contains: "playwright"
    - path: "___/accounts/scraper.py"
      provides: "RegistraduriaScraper class with browser management"
      exports: ["RegistraduriaScraper"]
      min_lines: 40
  key_links:
    - from: "___/accounts/scraper.py"
      to: "playwright.sync_api"
      via: "import sync_playwright"
      pattern: "from playwright.sync_api import"
---

<objective>
Install Playwright with Chromium browser and create base scraper class with browser singleton pattern.

Purpose: Establish headless browser infrastructure that Phase 13 Plan 02 will use for actual scraping logic.
Output: Working Playwright installation and RegistraduriaScraper class with browser lifecycle management.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-playwright-scraper/13-CONTEXT.md
@.planning/phases/13-playwright-scraper/13-RESEARCH.md

# Relevant source
@___/accounts/models.py (CedulaInfo model with Status choices)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Playwright and Chromium browser</name>
  <files>requirements.txt</files>
  <action>
Add playwright to requirements.txt (append after existing dependencies).

Then run:
1. `pip install playwright` (or pip install -r requirements.txt)
2. `playwright install chromium` to download Chromium browser binary

Verify installation with: `python -c "from playwright.sync_api import sync_playwright; print('OK')"`

Note: playwright-stealth is NOT installed yet (per CONTEXT.md: minimal stealth initially, add later if needed).
  </action>
  <verify>
Run: `python -c "from playwright.sync_api import sync_playwright; print('Playwright import OK')"`
Run: `playwright --version` shows version number
  </verify>
  <done>Playwright installed and Chromium binary downloaded</done>
</task>

<task type="auto">
  <name>Task 2: Create RegistraduriaScraper class with browser singleton</name>
  <files>___/accounts/scraper.py</files>
  <action>
Create new file `___/accounts/scraper.py` with:

1. **Module docstring** explaining purpose (census data scraping from Registraduria)

2. **Constants at top:**
   - `REGISTRADURIA_URL = 'https://consultacenso.registraduria.gov.co/consultar/'`
   - `DEFAULT_TIMEOUT = 90000` (90 seconds in milliseconds, per CONTEXT.md)
   - `RATE_LIMIT_SECONDS = 5` (minimum between requests, per SCRP-07)

3. **RegistraduriaScraper class** with:
   - Class-level `_playwright = None` and `_browser = None` for singleton
   - `get_browser(cls)` classmethod: lazy initialization of browser
     - Uses `sync_playwright().start()`
     - Launches chromium with `headless=not settings.DEBUG` (headed in DEBUG mode)
     - Stores in class-level variables
   - `close_browser(cls)` classmethod: cleanup method
     - Closes browser if exists
     - Stops playwright if exists
     - Resets class variables to None
   - `scrape_cedula(self, cedula: str) -> dict` method:
     - Creates new browser context via `self.get_browser().new_context()`
     - Wraps in try/finally that closes context
     - For now, returns `{'status': 'not_implemented'}` as placeholder
     - Phase 13 Plan 02 will implement actual scraping logic

4. **Logging setup:**
   - `logger = logging.getLogger('django-q')` (integrates with existing config)

5. **Imports:**
   - `from django.conf import settings`
   - `from playwright.sync_api import sync_playwright`
   - `import logging`

Structure should allow easy addition of playwright-stealth later (per CONTEXT.md: "code structure should be modular").
  </action>
  <verify>
Run: `python manage.py shell -c "from accounts.scraper import RegistraduriaScraper; print('Import OK')"`
Run: `python manage.py shell -c "from accounts.scraper import RegistraduriaScraper; s = RegistraduriaScraper(); print(s.scrape_cedula('12345678'))"`
Should print: `{'status': 'not_implemented'}`
  </verify>
  <done>RegistraduriaScraper class created with browser singleton pattern and context management</done>
</task>

</tasks>

<verification>
1. Playwright imports without error
2. Chromium browser binary exists (playwright install succeeded)
3. RegistraduriaScraper can be imported from accounts.scraper
4. Calling scrape_cedula returns placeholder dict (actual implementation in Plan 02)
5. Browser launches in headed mode when DEBUG=True
</verification>

<success_criteria>
- Playwright 1.50+ installed in requirements.txt
- Chromium browser binary downloaded
- RegistraduriaScraper class with get_browser(), close_browser(), scrape_cedula() methods
- Browser singleton pattern with lazy initialization
- Context cleanup in finally block
- Ready for Plan 02 to implement actual scraping logic
</success_criteria>

<output>
After completion, create `.planning/phases/13-playwright-scraper/13-01-SUMMARY.md`
</output>
